%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\chapter{Introduction}
\label{ch:Introduction}

Research in deep learning has produced an abundance of highly influential work in recent years, like
\glspl{cnn} \cite{lecun1998gradient}, transformers \cite{vaswani2017attention} and \glspl{gan} \cite{goodfellow2020generative}.
Machine learning models are becoming more accurate, achieving or surpassing human-level performance in visual
perception and natural language understanding. Because of their high innovation potential, machine learning models are increasingly
being used in real-world applications. To profit from this, many technology
enterprises offer services to ease training, development, and deployment of machine learning applications. These services can
be grouped under the term \gls{mlaas}. \par
While \gls{mlaas} is crucial to increase the accessibility of machine learning, it also poses a security threat. In recent years, numerous research papers
have been published that demonstrate how easily machine learning models can be extracted from \gls{mlaas} services \parencite{papernot2017practical,tramer2016stealing,
reith2019efficiently}. The standard procedure to steal such a machine learning model is to train a local
clone (also called substitute model) of the model hosted on the \gls{mlaas} service (also called target model). The substitute model is trained
using a dataset collected by the attacker. This dataset is also called the thief dataset. Labels for the thief dataset are obtained by querying the target
model on the thief dataset. The process of extracting machine learning models from \gls{mlaas} services is also called model stealing. \par
Because of the dramatic consequences of model stealing attacks, researchers have investigated how to defend against them. Two recent model stealing
defenses are prediction poisoning \cite{orekondy2019prediction} and \gls{prada} \cite{juuti2019prada}. Researchers have proposed attacks that evade
these defense strategies, despite the effort to
defend against model stealing attacks. One notable example is the model
extraction framework ActiveThief \cite{pal2020activethief}, which successfully evades the defense strategy  \gls{prada}. ActiveThief utilizes active
learning to determine which samples of the thief dataset it should query the target model on. Active learning is an intensively studied research
field that aims to minimize the labeling effort in the training process of machine learning models. However, the problem with active learning is
that it needs large amounts of computing resources. In this work, we extend the ActiveThief framework by using continual active learning in the model
extraction process.\par
Continual learning is a research field that aims to make machine learning models more robust against the advent of new data. The major problem
of the classic training procedure is that a model trained on a new task rapidly loses the ability to perform any previous tasks it was trained on.
Research in continual learning aims to develop methods that allow machine learning models to learn new tasks without forgetting the knowledge of old 
tasks. \par
\textbf{Delimitation} \hspace{0.2cm} We conduct all work in this thesis in the context of image classification. Furthermore, we conduct all experiments
with \glspl{cnn}. Therefore, our findings only apply to deep learning within the computer vision domain and may not generalize to other settings. \par
\textbf{Contributions} \hspace{0.2cm} The objective of this thesis is to combine existing approaches from continual learning
with approaches in the active learning domain. More specifically, we focus on regularization-based continual learning methods,
whereas we use uncertainty-based and diversity-based active learning methods. The continual learning methods we use are \gls{ewc} \cite{kirkpatrick2017overcoming},
\gls{mas} \cite{aljundi2018memory}, \gls{imm} \cite{lee2017overcoming} and \gls{alasso} \cite{park2019continual}.
Concerning active learning, we compare random sampling, \gls{lc} \cite{lewis1995sequential}, \gls{badge} \cite{ash2019deep}, \gls{bald} \cite{houlsby2011bayesian} and
CoreSet \cite{sener2017active}.
We analyze how the continual learning methods can speed up the active learning process, which combinations of continual learning and active learning methods
are most effective, and the trade-off between accuracy and speed up. While the focus of this thesis is on regularization-based continual
earning methods, we briefly explore the effectiveness of exemplar rehearsal continual earning using \gls{a-gem} \cite{chaudhry2018efficient}
combined with representation-based active learning in form of \gls{vaal} \cite{sinha2019variational}. Furthermore, we evaluate the performance of a custom Replay strategy.\par
After exploring the effectiveness of the different combinations of continual and active learning methods, we apply these combinations in the model stealing
domain. More specifically, we build upon the model extraction framework ActiveThief \cite{pal2020activethief} and investigate the performance of continual
active learning methods in the model stealing domain.