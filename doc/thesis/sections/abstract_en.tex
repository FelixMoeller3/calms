%% LaTeX2e class for student theses
%% sections/abstract_en.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\Abstract
The advancement of Machine Learning and especially Deep Learning has paved the way for remarkable breakthroughs in the Computer Vision domain \cite{He_2016_CVPR} \cite{goodfellow2014generative} \cite{6795724}.
Despite its benefits, Deep Learning is a resource-intensive task because the architecture of state-of-the-art models is becoming increasingly complex and these models need to be trained on vast amounts of labeled data.
Big-tech companies like Google, Amazon and Microsoft however have access to a lot of computing power which they monetize by offering Machine-Learning as a Service (MLaaS). MLaaS-Plattforms allow customers
to train their models in the cloud and make them publicly accessible via prediction APIs. While MLaaS makes Machine Learning more attainable it also poses a security risk. Previous works have shown that it is possible
 to infer numerous properties (e.g, training data \cite{shokri2017membership} and architecture \cite{oh2019towards}) of the target model, i.e. the model behind the prediction API \cite{tramer2016stealing} \cite{papernot2017practical}.
 However, we purely focus on stealing the \textit{functionality} of the target model, i.e. its ability to predict the correct class of a given input image. Because stealing Machine Learning models is such a pressing issue, there has been
 ongoing research on how to defend against model stealing attacks \cite{orekondy2019prediction} \cite{juuti2019prada}. To further develop 