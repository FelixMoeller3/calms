%% LaTeX2e class for student theses
%% sections/abstract_en.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28
\Abstract
The advancement of machine learning and especially deep learning has paved the way for remarkable breakthroughs in the computer vision domain 
\parencite*{goodfellow2020generative,he2016deep,lecun1989backpropagation}. Despite its benefits, deep learning is a demanding task that requires
significant resources due to the increasingly complex architecture of state-of-the-art models and the need for extensive training with labeled data.
Technology companies like Google, Amazon and Microsoft, however, have access to a lot of computing power, which they monetize by offering \gls{mlaas}.
\gls{mlaas} platforms allow customers to train their models in the cloud and make them publicly accessible via prediction \glspl{api}. 
While \gls{mlaas} makes machine learning more attainable, it poses a security risk. Previous works have shown that it is possible to infer numerous
properties (e.g., training data \cite{shokri2017membership} and architecture \cite{oh2019towards}) of the model behind the prediction \gls{api}
\parencite{papernot2017practical,tramer2016stealing}.
However, we purely focus on stealing the \textit{functionality} of the target model, i.e., its ability to predict the correct class of a given input image.
In this thesis, we build upon ActiveThief \cite{pal2020activethief}, a model extraction framework using active learning strategies to steal deep neural networks. 
First, we propose a novel approach named continual active learning, which combines active learning and continual learning to improve the trade-off between
accuracy and efficiency in the active learning process. Our evaluation demonstrates that continual active learning reduces validation accuracy by 10-30\%
while reducing execution time by 60-98\%. Next, we incorporate continual active learning into the ActiveThief framework and investigate whether our findings
from traditional continual active learning are transferable to model stealing. We find that continual active learning falls short of the performance achieved by ActiveThief,
specifically when training with the predicted label of the target model. Furthermore, we show that continual active learning benefits from training with the softmax
output of the target model, which has been observed for previous model stealing attacks \parencite{pal2020activethief,orekondy2019knockoff}.
