%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\chapter{Related Work}
\label{ch:Related_work}
Related work for this thesis can be grouped into three different categories:
The first category is \href{sec:Related_work:Active_Learning}{Active Learning}.
Active Learning is a special form of machine learning where an oracle is present
which can label arbitrary data points. A Machine Learning model trained using Active
Learning iteratively queries the oracle with unlabeled data points, trains a new model
and determines which data should be queried next. The second category is
\href{sec:Related_work:Continual_Learning}{Continual Learning}. Continual learning is a
machine learning technique which aims to make a given machine learning model learn new tasks
without forgetting the knowledge of previous tasks. 
The third category is \href{sec:Related_work:Model_Stealing}{Model Stealing}. Model Stealing is the
process of strategically querying a third-party machine learning model (also referred to as target model)
to train a local model (also referred to as substitute model) which is supposed to approximate the target model as
good as possible.

% use Settles as reference for the general introduction
% Mention that active learning is a specific form of semi-supervised learning (according to Mundt et al.)
\section{Active Learning}
\label{sec:Related_work:Active_Learning}
\subsection{General Introduction}
Active Learning is a specific form of machine learning where the learner can query an oracle to label arbitrary data points.
The motivation behind Active Learning is that nowadays it is not difficult to obtain large amounts of data, but the bottleneck
is assigning labels to them. Active Learning aims to overcome this issue by producing a highly accurate model with little amount
 of labeled data. The idea is that a learner which chooses the data it is trained on should perform better or at least as good
 as a model trained on all available data.
\subsection{Active Learning Approaches used in the Experiments}
\subsubsection{Least Confidence}
\subsubsection{CoreSet}
\subsubsection{BALD}
\subsubsection{Badge}
% Structure: First some general introduction then present
% the different approaches used in the experiments (LC, CoreSet, BALD, Badge)

% Use parisi et al. as reference for the general introduction
% Synaptic Intelligence paper also gives good introduction into approaches and structures them
% into three categories
% Also mention the difference between task-incremental, class-incremental and domain-incremental learning
\section{Continual Learning}
\label{sec:Related_work:Continual_Learning}
\subsection{General Introduction}
Artificial Neural Networks suffer from a problem called \enquote{Catastrophic Forgetting} \cite{mccloskey1989catastrophic}.
Catastrophic Forgetting is the phenomenon that the performance of a neural network previously trained on task $t_{n-k}$
severely decreases when the same neural network is later trained on task $t_n (k>0)$. More briefly, neural networks struggle
to retain the knowledge of previous tasks when learning new tasks. The problem of Catastrophic Forgetting has wide-ranging
implications for the use of neural networks because it limits their use to settings where the data at deployment time is
distributed identically to the data observed at training time. Clearly, this is not the case in most real-world applications.
Research in Continual Learning aims to develop mechanisms which alleviate Catastrophic Forgetting. The Continual Learning approaches
which have been proposed so far can be grouped into three categories according to \cite{mundt2020wholistic} and
\cite{zenke2017continual}. Mundt et al. \cite{mundt2020wholistic} propose to group Continual Learning approaches into
\textbf{regularization}, \textbf{rehearsal} and \textbf{architectural} approaches whereas Zenke et al. \cite{zenke2017continual}
group Continual Learning approaches into \textbf{architectural},\textbf{functional} and \textbf{structural} approaches. In the
following, we will stick to the categorization proposed by Mundt et al. because it is broader and fully encompasses the
categorization by Zenke et al.
\subsubsection{Regularization}
Regularization-based approaches to Continual Learning aim to prevent the forgetting of previous tasks by adding a regularization
term to the model's loss function. The regularization term is used as a proxy for how much the performance of the model on previous
tasks will decrease, i.e. a high regularization term indicates that the model will perform poorly on the old tasks with the current
weights and a low regularization term indicates that the model has not lost much knowledge of the old tasks. The way the
regularization term is computed can further be divided into two groups. There are \textbf{structural} approaches which regularize
based on weight changes to the model and there are \textbf{functional} approaches which regularize based on the output of the model.
Notable examples of structural approaches include Elastic Weight Consolidation (EWC) \cite{kirkpatrick2017overcoming},Memory Aware
Synapses (MAS) \cite{aljundi2018memory}, Incremental Moment Matching (IMM) \cite{lee2017overcoming} as well as Asymmetric Loss
Approximation by Single-Side Overestimation (ALASSO) \cite{park2019continual} which is an extension of Synaptic Intelligence (SI)
\cite{zenke2017continual}. All the structural regularization approaches will be covered in more detail in the section on
\href{sec:Related_work:Continual_Learning:Experiments}{the continual learning approaches used in the experiments}. \\
Functional regularization approaches are inspired by knowledge distillation \cite{hinton2015distilling}. They add a distillation
loss to the objective function which is computed based on the prediction of a data sample stored for future use. These data samples
are called soft targets. Li et al. \cite{li2017learning} compute the distillation loss by using the output of the newly arrived task
given by the model trained on the old tasks. The distillation loss they introduce aims to retain the prediction of the old model on
the new task even if the prediction itself may be inaccurate. The approach of Rannen et al.
\cite{rannen2017encoder}, called Encoder Based Lifelong Learning (EBLL) is based on the approach of Li et al., however in EBLL the
distillation loss is computed based on autoencoder reconstructions of old tasks.
\subsection{Continual Learning Approaches used in the Experiments}
\label{sec:Related_work:Continual_Learning:Experiments}
\subsubsection{EWC}
\subsubsection{MAS}
\subsubsection{ALASSO}
\subsubsection{IMM}

% Structure: First some general introduction then present
% the different approaches used in the experiments (EWC, MAS, ALASSO, IMM, potentially Replay?)


\section{Model Stealing}
\label{sec:Related_work:Model_Stealing}
% Hier Active Thief erwähnen und falls später noch defense strategies verwendet werden das ebenfalls noch erwähnen

\dots
%% ---------------------
%% | / Example content |
%% ---------------------