%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\chapter{Background}
\label{ch:Background}

In this chapter we will give an overview of the background knowledge that is needed to understand the following chapters. The topic
of this thesis is at the intersection of the fields of Active Learning, Continual Learning and Model Stealing and therefore we will
give an introduction to each of these fields. 

\section{Active Learning}
\label{sec:ActiveLearning}

%TODO: Hier learner-Beschreibung anpassen
Active Learning is a subfield of Machine Learning that focuses on the problem of how to select the most informative data points to label.
Research in Active Learning is motivated by the fact that data acquisition is often easy because it can be automated whereas labeling is
difficult and time-consuming. Therefore, the goal of research in Active Learning is to develop methods which maximize model performance
with minimal amount of labeled data. A typical Active Learning scenario comprises a learner, an oracle, unlabeled data and labeled data.
The learner is the actual algorithm which selects which data points to label by the oracle. Let $I$ be the instance space (i.e. the set
of all possible data points) and $L$ be the label space (i.e. the set of all possible labels). The oracle $O$ can be seen as the function
\begin{equation}
    O: I \rightarrow L, x \mapsto y(x)
\end{equation}
where $y(x)$ is the true label of the data point $x$, often just referred to as $y$. The unlabeled data $U$ is a subset of $I$ just as the set
of labeled data $L$. At the beginning there are no labeled data points. Often a few labeled data points are randomly sampled from $U$ and labeled
by the oracle to initialize the learning process. A detailed overview on research activities within the Active Learning Domain is presented in
\cite{settles2009active}. Note however that more recent works are not contained in this literature survey because it was last updated in 2009.
In general, Active Learning methods can be divided into three categories:
\begin{itemize}
    \item \textbf{Query Synthesis Active Learning}
    \item \textbf{Pool-based Active Learning}
    \item \textbf{Stream-based Active Learning}
\end{itemize}
The taxonomy mentioned above is composed in a data-centric way. While researchers generally agree on the taxonomy mentioned above,
it is important to note that the effectiveness of an active learning strategy also depends on the type of Machine Learning Model that is used
(e.g. (Convolutional) Neural Networks, Support Vector Machines, etc.) and the type of data that is used (e.g. images or text).
For example, the pool-based Active Learning strategy CoreSet \cite{sener2017active} was specifically designed for CNNs.

\subsection{Query Synthesis Active Learning}
\label{sec:QuerySynthesisActiveLearning}
Query Synthesis Active Learning, also known as Membership Query Synthesis Active Learning, was among the first active learning scenarios
that were proposed \cite{angluin1988queries}. The idea behind this approach is to synthesize data points from the input space, rather than
to sample real data points. Nowadays, this is done by training a generative model (e.g a Generative Adversarial Network) \cite{zhu2017generative}
which learns the distribution of unlabeled data. However, earlier works relied on statistical models such as Gaussian Mixture Models 
\cite{cohn1996active}. The generated queries are then labeled by the oracle and can be used to train the Machine Learning Model. Note that
Query Synthesis is not limited to classification tasks. For example, \cite{cohn1996active} proposed a method to predict the absolute coordinates
of a robot hand when given the joint angles of its mechanical arms as inputs. When the oracle is a human annotator, Query Synthesis Active Learning
researchers have encountered problems in labeling them. Because the generated queries do not show any class-discriminative features, human annotators
struggled to assign any class to them in the survey of \cite{baum1992query}.

% TODO: Hier ein Bild wie QS Active Learning funktioniert einf√ºgen

\subsection{Pool-based Active Learning}
\label{sec:PoolBasedActiveLearning}
Pool-based Active Learning is the most widely studied and used type of Active Learning. The idea behind pool-based Active Learning approaches
is to iteratively select the most informative data points from the current unlabeled pool, query the oracle for their labels and add them to the
labeled pool. Next, the ML model is trained on the current labeled pool. This process is repeated until the query budget is exhausted. A more detailed
explanation can be found in \href{alg:PoolBasedActiveLearning}{Algorithm 1}. Pool-based Active Learning strategies share this structure among each other.
The main difference between them is the informative measure, i.e. the criterion with which they select which data points to label next. Within the 
Pool-based Active Learning category, there are two subcategories: \textbf{Uncertainty-based} sampling and \textbf{Diversity-based} sampling. 
Uncertainty-based sampling strategies select the data points that the model is most uncertain about. Diversity-based sampling strategies on the other
hand aim to select data points that best represent the data distribution in the unlabeled pool. All the active learning strategies that we will use
are pool-based Active Learning Strategies, stemming both from the Uncertainty-based and Diversity-based subcategories.

\begin{algorithm}
    \caption{Pool-based Active Learning} \label{alg:PoolBasedActiveLearning}
    \begin{algorithmic}[1]
        \Require Unlabeled data $U$,Labeled data $L = \emptyset$:, Oracle $O$, Model $M$, budget $B$
        \State Select $k$ data points from $U$ at random, obtain labels by querying $O$ and set $L=\{x_1,\ldots,x_1\}$
        and $U = U \setminus \{x_1,\ldots,x_1\}$ \Comment{Initialization}
        \State Train $M$ on initial labeled set $L$
        \While{label budget $B$ not exhausted}
            \State Select $l$ data points from $U$ predicted to be the most informative by the Active Learning strategy
            \State Set $L= L \cup \{x_i,\ldots,x_l\}$ and $U = U \setminus \{x_i,\ldots,x_l\}$
            \State Train $M$ on labeled set $L$
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\subsection{Stream-based Active Learning}
\label{sec:StreamBasedActiveLearning}
Stream-based Active Learning is closer to Pool-based Active Learning than Query Synthesis Active Learning. It was first introduced by Cohn et al. 
\cite{cohn1994improving}. The main difference between Stream-based Active Learning and Pool-based Active Learning is that data arrives sequentially
instead of having a batch of data points at once. In the stream-based Active Learning scenario, the learner draws a data point from the data source
one at a time. For each data point, the learner can then decide to query the oracle for its label or to discard it. The decision whether to label a
data point can either be made on the basis of its informativeness \cite{dagan1995committee} or its location within the instance space \cite{cohn1994improving}.
In the latter case the learner would label the data point if it is located in a region of the instance space that the learner is not confident about.

\section{Continual Learning}
\label{sec:ContinualLearning}
Continual Learning is a subfield of Machine Learning that aims to solve the problem of catastrophic forgetting. To elaborate further on this problem,
it is important to remember that most machine learning services are deployed in an environment where constant changes occur. To adapt to this, it is
necessary for Machine Learning Models to learn continually, i.e. to learn new tasks without forgetting the knowledge they have acquired in the past.
This is a common case for humans. For example, if a child has once learned to walk, it does not forget how to walk when it learns to ride a bike.
In contrast to human behavior, the performance of Machine Learning Models on old tasks rapidly decreases when they are trained on new tasks. This phenomenon
is known as \enquote{Catastrophic Forgetting} and was already discovered in the early days of Machine Learning research \cite{mccloskey1989catastrophic}.
In more general terms, research in Continual Learning is looking not only to alleviate Catastrophic Forgetting, but to solve the 
\enquote{stabiliy / plasticity dilemma} \cite{carpenter1988art}. The stability / plasticity dilemma refers to the fact that Machine Learning Models
should ideally be stable enough to retain their performance on old tasks while simultaneously being plastic enough to adapt to new tasks. This is a dilemma
because both properties are designed even though they are in conflict with each other. In practice, Machine Learning Models are rather plastic than stable.
This is especially true for Deep Neural Networks, but generally for all Machine Learning Models that are trained by greedily updating their parameters using
gradient descent \cite{mundt2020wholistic}. \\
Continual Learning is mainly used in scenarios where new tasks arrive over time or where the data distribution changes over time. Despite being useful
in these classic scenarios, Continual Learning approaches can also be used in cases where the data cannot be stored for legal reasons or due to memory
constraints, i.e. when the normal batch learning approach with a large training set cannot be applied. 


\subsection{Continual Learning Scenarios}
\label{sec:ContinualLearningScenarios}
Continual Learning is a rapidly evolving research field and terminology and taxonomy are still being established. Among the most important factors to distinguish
is the way in which new tasks arrive. In the following, we will introduce the three typical Continual Learning Scenarios as established in . 

\subsubsection{Task-Incremental Continual Learning}
\label{sec:TaskIncrementalContinualLearning}
In the Task-Incremental setting, a model is informed

\subsubsection{Domain-Incremental Continual Learning}
\label{sec:DomainIncrementalContinualLearning}

\subsubsection{Class-Incremental Continual Learning}
\label{sec:ClassIncrementalContinualLearning}

\subsection{Continual Learning Approaches}
\label{sec:ContinualLearningApproaches}

\subsubsection{Architectural Approaches}
\label{sec:ArchitecturalApproaches}

\subsubsection{Functional Approaches}
\label{sec:FunctionalApproaches}

\subsubsection{Structural Approaches}
\label{sec:StructuralApproaches}

\section{Model Stealing}
\label{sec:ModelStealing}