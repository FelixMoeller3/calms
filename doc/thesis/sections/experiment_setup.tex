%% LaTeX2e class for student theses
%% sections/methodology.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.6, 2022-09-28

\chapter{Experiment Setup}
\label{ch:ExperimentSetup}
% Describe hardware and software specification, datasets used, models used.
% Specifically mention which datasets are target datasets for model stealing
% and which are thief datasets
The findings of this thesis rely heavily on thorough experimentation. To reproduce our findings, we list in detail the
conditions under which we conducted the experiments. This includes training hyperparameters, datasets, Neural Network Architectures
and details of the code and libraries used. In general, our experiments can be divided into two categories: Experiments that explore
the classic Continual Active Learning Setting as in \ref{sec:Methodology:CombiningCLandAL} and experiments that explore Continual 
Active Learning for Model Stealing as in \ref{sec:Methodology:CombiningCLandALforMS}. We perform this differentiation because many 
training hyperparameters, datasets and Neural Network Architectures depend on the whether classic Continual Learning or Continual 
Active Learning for Model Stealing is investigated. First, we mention the general setup, including Hardware, Software, all datasets used
and the training hyperparameters shared between the two categories of experiments. Next, we go into detail about the special setup for
Continual Active Learning and Continual Active Learning for Model Stealing.

\section{General Experiment Setup}
\label{sec:ExperimentSetup:FirstSection}
%TODO: Some introductory words about the general setup

\subsection{Hardware}
\label{sec:ExperimentSetup:Hardware}
All experiments are conducted on the High Performance Computing (HPC) cluster bwUniCluster 2.0 \cite{bwUnicluster}. bwUniCluster 2.0 is an HPC cluster
funded by the Ministry of Science, Research and the Arts Baden-Württemberg and the Universities of the State of Baden-Württemberg. It currently consists
of more than 840 compute nodes with each node falling one of the following categories: \enquote{Thin}, \enquote{HPC}, \enquote{IceLake}, \enquote{Fat},
\enquote{GPUx4}, \enquote{GPUx8}, \enquote{GPUx4 A100}, \enquote{GPUx4 H100} and \enquote{Login}. In our experiments, we use the nodes GPUx4, GPUx8 and
GPUx4 A100. Their hardware specifications can be found in Appendix \ref{sec:Appendix:Specifications}.

\subsection{Software and Libraries}
\label{sec:ExperimentSetup:Software}
All the code used in this thesis is written in version 3.9.4 of the programming language Python \cite{Rossum1995Python}. Furhermore, we use the deep
learning Library PyTorch \cite{paszke2019pytorch} (version 1.13.1) to implement both Active and Continual Learning Algorithms. All further libraries
and their respective versions can be found in Appendix \ref{sec:Appendix:Specifications}.

\subsection{Continual Learning Strategies}
\label{sec:ExperimentSetup:CLStrategies}
In our experiments, we use the Continual Learning Strategies EWC, MAS, IMM and Alasso. For details on these Continual Learning strategies we refer
the reader to section \ref{sec:Related_work:Continual_Learning:Experiments}. As a baseline, we used the naive strategy of performing classic
gradient descent without any regularization. In the following we will refer to this strategy as \enquote{Naive}. \par
Following the paper introducing MAS \cite{aljundi2018memory}, we set the regularization parameter $\lambda$ to 1.0 for MAS. The choice of this parameter
is crucial because it enables a sound evaluation. If we trivially chose to set $\lambda$ to 0, then there would be no difference between MAS and the naive
strategy. On the other hand, fine-tuning neither possible (we use more than 25 combinations of Continual Learning strategies and Active Learning strategies 
on 3 datasets) nor fair (to evaluate the quality and effect of all Continual Learning strategies they should be tested under the same conditions, which
includes the same balance between old and new tasks). We therefore set $\lambda$ in EWC and IMM to 1.0 accordingly. The regularization parameter $c$ in
Alasso, which is equivalent to $\lambda$ in EWC, MAS and IMM is set to 0.5 for all experiments. We tried setting $c$ to 1.0, however we noticed that this
led to divergence during gradient descent despite employing heavy gradient clipping. Since employing stronger gradient clipping was not a viable solution
because it impedes model convergence within reasonable time, we decided to relax the regularization parameter $c$ to 0.5. We assume that the convergence 
problems Alasso faces are due to the fact that $\alpha$ in equation 3.15 makes the parameter space non-convex. \par
As mentioned before, we employ gradient clipping to avoid gradient descent from diverging. Not only is gradient clipping a viable remedy against the exploding
gradient problem, it has also been shown that it can accelerate the training process \cite{zhang2019gradient}. In our implementation we clip gradients by their
$l2$-Norm. Across all Continual Learning procedures, including Naive, we clip the gradients to a maximum $l2$-Norm of 20.0 to accelerate the training process.
When conducting experiments with MAS and Alasso we encountered exploding gradient problems, which we further investigated for MAS and Alasso separately. While we
aimed to eliminate the exploding gradient problem, which is mitigated more by clipping smaller gradient, we did not want to restrict the model too much to hinder 
model convergence. After carefully exploring with different values to clip at, we found setting the threshold for the $l2$-Norm to 2.0 to be effective, mitigating
the exploding gradient problem while simultaneously enabling model convergence. \par
Apart from the parameter weighting the regularization term, the Continual Learning strategies do not share any hyperparameters. EWC in fact does not have
any further hyperparameters and neither does MAS. On the other hand, for IMM we have the choice between mean-IMM and mode-IMM. Furthermore, we can choose
to apply Weight-transfer, L2-transfer as well as Dropout-transfer, and we can choose values for the $\alpha$ Parameter. In our setup we use mean-IMM, Weight-
Transfer and L2-transfer and set the $\alpha$ parameter to [0.45,0.55] (i.e. $\alpha_1 = 0.45, \alpha_2 = 0.55$), as suggested by the authors. For Alasso, we set
the parameter $a$, which controls the overestimation on the unobserved side, to 3.0. Following the recommendation of the authors, we perform parameter decoupling
for the $\Omega$ updates and therefore set $a'$ to 1.5 and $c'$ to 0.25. \par
A detailed summary of the hyperparameters used can be found in Appendix \ref{sec:Appendix:Specifications}.


\subsection{Active Learning Strategies}
\label{sec:ExperimentSetup:ALStrategies}
In our experiments, we use the Continual Learning Strategies Badge, LC, CoreSet and BALD. For details on these Active Learning strategies we refer
the reader to section \ref{sec:Related_work:Active_Learning:Approaches}. For Badge, we use the Euclidean distance in the $k$-means++ algorithm.
For CoreSet we use the Euclidean distance between the activations of the penultimate layer of the neural network as a distance metric for $k$-Center
algorithm. For BALD we use the Monte Carlo dropout with $T=25$ samples. We omit Monte Carlo dropout in cases where the model does not contain dropout
layers as the prediction of such a model is deterministic. LC does not contain any hyperparameters, so we do not have to specify any. \par
Across all Active Learning strategies we use the same initial budget, i.e. the number of points we sample from the training set before the first iter-
ation of the Active Learning algorithm, as the batch size.

\subsection{Datasets}
\label{sec:ExperimentSetup:Datasets}
In our experiments, we use the datasets MNIST \cite{mnist_web}, Fashion-MNIST \cite{xiao2017fashion}, CIFAR-10 \cite{cifar},
CIFAR-100 \cite{cifar}, Tiny-ImageNet \cite{le2015tiny} and a subset of the ILSVRC2012-14 dataset \cite{imagenet}. The subset of the ILSVRC2012-14 dataset that we
use is the first of 10 training batches downloaded from \url{http://www.image-net.org/data/downsample/Imagenet32_32.zip}. From now on, we will refer to this subset
of the ILSVRC2012-14 dataset as \enquote{SmallImagenet}. For all datasets, we use the standard train test split as proposed by Pytorch. We rescale all images to 32x32
pixels because for the success of a model stealing attack the image shape of the thief dataset has to match the image shape accepted by the target model. We will cover
this in more detail in section \ref{sec:Methodology:CALMSsetup}. Moreover, we normalize the train and test split of all datasets by using the mean and standard deviation
of the training set. For all datasets apart from MNIST and FashionMNIST, we further apply Data Augmentation. We use random horizontal flips with a probability of 0.5
followed by random cropping. A detailed list of the datasets used can be found in Appendix \ref{sec:Appendix:Specifications}. \par

\subsection{Neural Network Architectures}
\label{sec:ExperimentSetup:NNArchitectures}
In our experiments, we use the neural network architectures Resnet18 \cite{he2016deep} and CNN Architectures from the ActiveThief paper \cite{pal2020activethief}.
In the following, we will refer to this CNN architecture as \enquote{ActiveThiefConv}. Since there are multiple variations of the architecture, more specifically ones with
2,3 or 4 convolutional blocks we will refer to those as ActiveThiefConv2, ActiveThiefConv3 and ActiveThiefConv4 respectively.
%TODO hier Archtitekturen einfügen und beschreiben


\subsection{Training Hyperparameters}
\label{sec:ExperimentSetup:Hyperparameters}
% TODO: Überlegen, ob es noch mehr Hyperparameter gibt, die wir erwähnen sollten
We use PyTorch's SGD optimizer with a learning rate of 0.1 across all experiments. Furthermore, we schedule the learning rate by a factor of 0.1, but the number of
epochs after which we schedule the learning rate differs between experiments. It is important to note that we re-instantiate the SGD optimizer after each Active Learning
iteration. We will go more into detail on this in the description of the respective experiments. Apart from scheduling the learning rate, we use momentum \cite{cutkosky2020momentum}
of 0.9 and $l2$-regularization of 0.0005. For all experiments we use a batch size of 128 and shuffle the entire training set before each epoch.

\subsection{Randomness}
\label{sec:ExperimentSetup:Randomness}
%TODO: hier über Random number generation sprechen, erwähnen, dass kein fixed seed gewählt wird, und welche NN Initialisierung gewählt wird

\section{Special Setup for Continual Active Learning}
\label{sec:Methodology:CALsetup}
We experiment with Continual Active Learning using Resnet18 as our Neural Network Architecture and CIFAR-10 as our dataset. We perform experiments with a Batch Size of 1000,
2000 and 4000 respectively. Regardless of the batch size, we perform Active Learning until the unlabeled pool is exhausted. This means that we perform Active Learning for
49, 24 and 12 iterations respectively. For batch sizes 1000 and 2000 each query consists of 2000 points, while the last query for batch size 4000 consists of 2000 points.
For batch size 2000 and 4000 we train for 150 epochs per iteration, decaying the learning rate by a factor of 10 after 80 and 120 epochs respectively. For batch size 1000 we
train for 80 epochs and decay the learning rate by 10 after 60 epochs. To compare the performance of Continual Active Learning with pure Active Learning, we computed the results
for Active Learning with the same batch sizes as for Continual Active Learning. In this experiment setup, we use warm start and train for 200 epochs in each iteration. Here, we
decay the learning rate by 10 after 100 and 150 epochs respectively.

\section{Special Setup for Continual Active Learning for Model Stealing}
\label{sec:Methodology:CALMSsetup}
When transferring Continual Active Learning to the Model Stealing domain, we change our setup compared to Continual Active Learning previously. Instead of using Resnet18 as our
model architecture, we use the CNN architecture from the ActiveThief paper \cite{pal2020activethief}, mainly because we want to compare our Continual Active Learning approach to
the framework proposed by the authors of ActiveThief. For most of our experiments, we use ActiveThiefConv3 apart from those experiments which investigate the effect of the model 
architecture on Model Stealing Attacks. We train the target models on the datasets CIFAR-10, CIFAR-100 and MNIST. We train the target models on CIFAR-10 and CIFAR-100 for
150 epochs using momentum of 0.9 and $l2$-regularization of 0.0005 without learning rate decay. When training the target models on MNIST, we change the number of epochs to 50.
For all Model Stealing attacks we do \textit{not} retrain the target model. Instead, we train one target model per dataset, save them to the disk and load them for all Model Stealing
attacks. This way we reduce the uncertainty in our experiments introduced by different weight initializations of the target model. During all Model Stealing attacks, we use SmallImageNet
as our thief dataset. \par
Similar to the classic Continual Learning setting, we compute a baseline with Active Learning for Model Stealing. For this baseline, we use Active Learning with a batch size of 1000
and a total budget of 20000 points. The models are trained using cold start for 200 epochs per iteration. We decay the learning rate by a factor of 10 after 100 and 150 epochs. \par
For Continual Active Learning for Model Stealing, we use a batch size of 2000 and a total budget of 40000. We decided to increase the batch size for Continual Active Learning compared
to pure Active Learning because we noticed a clear correlation between batch size and model performance when training using Continual Active Learning. We train the substitute model for
150 epochs per iteration and decay the learning rate by a factor of 10 after 80 and 120 epochs with momentum of and $l2$-regularization of 0.0005.
% TODO: Mention setup of training target models and the one for stealing
